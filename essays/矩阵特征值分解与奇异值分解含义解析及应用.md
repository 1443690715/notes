[<font size=4>←返回主目录<font>](../README.md)
</br></br></br>

## 特征值与特征向量的几何意义

矩阵的乘法是什么，别只告诉我只是“前一个矩阵的行乘以后一个矩阵的列”，还会一点的可能还会说“前一个矩阵的列数等于后一个矩阵的行数才能相乘”，然而，这里却会和你说――那都是表象。

矩阵乘法真正的含义是变换，我们学《线性代数》一开始就学行变换列变换，那才是线代的核心――别会了点猫腻就忘了本――对，矩阵乘法
<img src="http://www.forkosh.com/mathtex.cgi? \Small Y=AB">就是线性变换，若以其中一个向量A为中心，则B的作用主要是使A发生如下变化。

1.	伸缩

	```
	clf;
	A = [0, 1, 1, 0, 0;...
     	1, 1, 0, 0, 1];  % 原空间
	B = [3 0; 0 2];      % 线性变换矩阵

	plot(A(1,:),A(2,:), '-*');hold on
	grid on;axis([0 3 0 3]); gtext('变换前');

	Y = B * A;

	plot(Y(1,:),Y(2,:), '-r*');
	grid on;axis([0 3 0 3]); gtext('变换后');
	```

	![1]

	从上图可知，y方向进行了2倍的拉伸，x方向进行了3倍的拉伸，这就是B=[3 0; 0 2]的功劳,3和2就是伸缩比例。请注意，这时B除了对角线元素为各个维度的倍数外，非正对角线元素都为0，因为下面将要看到，对角线元素非0则将会发生切变及旋转的效果。

2.	切变

	```
	clf;
	A = [0, 1, 1, 0, 0;...
	     1, 1, 0, 0, 1];  % 原空间
	B1 = [1 0; 1 1];       % 线性变换矩阵
	B2 = [1 0; -1 1];       % 线性变换矩阵
	B3 = [1 1; 0 1];       % 线性变换矩阵
	B4 = [1 -1; 0 1];       % 线性变换矩阵

	Y1 = B1 * A;
	Y2 = B2 * A;
	Y3 = B3 * A;
	Y4 = B4 * A;

	subplot(2,2,1);
	plot(A(1,:),A(2,:), '-*'); hold on;plot(Y1(1,:),Y1(2,:), '-r*');
	grid on;axis([-1 3 -1 3]);
	subplot(2,2,2);
	plot(A(1,:),A(2,:), '-*'); hold on;plot(Y2(1,:),Y2(2,:), '-r*');
	grid on;axis([-1 3 -1 3]);
	subplot(2,2,3);
	plot(A(1,:),A(2,:), '-*'); hold on;plot(Y3(1,:),Y3(2,:), '-r*');
	grid on;axis([-1 3 -1 3]);
	subplot(2,2,4);
	plot(A(1,:),A(2,:), '-*'); hold on;plot(Y4(1,:),Y4(2,:), '-r*');
	grid on;axis([-1 3 -1 3]);
	```

	![2]

3.	旋转

	所有的变换其实都可以通过上面的伸缩和切变变换的到，如果合理地对变换矩阵B取值，能得到图形旋转的效果，如下，

	```
	clf;
	A = [0, 1, 1, 0, 0;...
	     1, 1, 0, 0, 1];  % 原空间
	theta = pi/6;
	B = [cos(theta) sin(theta); -sin(theta) cos(theta)];
	Y = B * A;
	figure;
	plot(A(1,:),A(2,:), '-*'); hold on;plot(Y(1,:),Y(2,:), '-r*');
	grid on;axis([-1 3 -1 3]);
	```
	![3]


好，关于矩阵乘就这些了。那么，我们再来看我们的主题，如果变换矩阵对某一个向量或某些向量只发生伸缩变换，不对这些向量产生旋转的效果，那么这些向量就称为这个矩阵的__特征向量__，伸缩的比例就是__特征值__。

现在，我们再规范一些（毕竟大家都不是学文科的）定义。


如果说一个向量v是方阵A的特征向量，将一定可以表示成下面的形式：

<img src="http://www.forkosh.com/mathtex.cgi? \Large Ax=\lambda{x}">

这时候λ就被称为特征向量v对应的特征值，特征值分解是将一个矩阵分解成下面的形式：

<img src="http://www.forkosh.com/mathtex.cgi? \Large A=Q\Sigma{Q^{-1}}">

其中Q是这个矩阵A的特征向量组成的矩阵，Σ是一个对角阵，每一个对角线上的元素就是一个特征值，这就是特征值分解公式。我们通过特征值分解得到的前N个特征向量，那么就对应了这个矩阵最主要的N个变化方向。我们利用这前N个变化方向，就可以近似这个矩阵（变换）。

特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么。

从线性空间的角度看，在一个定义了内积的线性空间里，对一个N阶对称方阵进行特征分解，就是产生了该空间的N个标准正交基，然后把矩阵投影到这N个基上。N个特征向量就是N个标准正交基，而特征值的模则代表矩阵在每个基上的投影长度。特征值越大，说明矩阵在对应的特征向量上的方差越大，功率越大，信息量越多。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。

在机器学习特征提取中，意思就是最大特征值对应的特征向量方向上包含最多的信息量，如果某几个特征值很小，说明这几个方向信息量很小，可以用来降维，也就是删除小特征值对应方向的数据，只保留大特征值方向对应的数据，这样做以后数据量减小，但有用信息量变化不大。PCA就是基于这种思路。

Matlab中通过eig函数就可求得特征值和特征向量矩阵。

```
>> B = [ 3     -2      -.9    2*eps
     -2      4       1    -eps
     -eps/4  eps/2  -1     0
     -.5    -.5      .1    1   ]
B =
    3.0000   -2.0000   -0.9000    0.0000
   -2.0000    4.0000    1.0000   -0.0000
   -0.0000    0.0000   -1.0000         0
   -0.5000   -0.5000    0.1000    1.0000

>> [V D] = eig(B)
V =
    0.6153   -0.4176   -0.0000   -0.1437
   -0.7881   -0.3261   -0.0000    0.1264
   -0.0000   -0.0000   -0.0000   -0.9196
    0.0189    0.8481    1.0000    0.3432
D =
    5.5616         0         0         0
         0    1.4384         0         0
         0         0    1.0000         0
         0         0         0   -1.0000
```

D对角线的元素即为特征值（表示了伸缩的比例），D就是特征值分解公式中的Q，V的每一列与D没列对应，表示对应的特征向量，即特征值分解中的Σ。


## 奇异值分解

特征值分解是一个提取矩阵特征很不错的方法，但是它只是对方阵而言的，在现实的世界中，我们看到的大部分矩阵都不是方阵，比如说有N个学生，每个学生有M科成绩，这样形成的一个N * M的矩阵就不可能是方阵，我们怎样才能描述这样普通的矩阵呢的重要特征呢？奇异值分解可以用来干这个事情，奇异值分解是一个能适用于任意的矩阵的一种分解的方法。



## 牛哄哄的应用



Google PageRank
PCA
PSD



[1]:../images/矩阵特征值分解与奇异值分解含义解析及应用/1.png
[2]:../images/矩阵特征值分解与奇异值分解含义解析及应用/2.png
[3]:../images/矩阵特征值分解与奇异值分解含义解析及应用/3.png
